{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 65713)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# wordvector's embedding-path\n",
    "embedding_path = './counter-fitted-vectors.txt' \n",
    "\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "with open(embedding_path, 'r') as ifile:\n",
    "    for line in ifile:\n",
    "        embedding = [float(num) for num in line.strip().split()[1:]]\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "embeddings = np.array(embeddings)\n",
    "print(embeddings.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all embedding-vector's l2 norm\n",
    "norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "# normalication\n",
    "embeddings = np.asarray(embeddings / norm, \"float32\")\n",
    "product = np.dot(embeddings, embeddings.T)\n",
    "np.save(('cos_sim_counter_fitting.npy'), product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n"
     ]
    }
   ],
   "source": [
    "idx2word = {}\n",
    "word2idx = {}\n",
    "\n",
    "print(\"Building vocab...\")\n",
    "with open('./counter-fitted-vectors.txt', 'r') as ifile:\n",
    "    for line in ifile:\n",
    "        word = line.split()[0]\n",
    "        if word not in idx2word:\n",
    "            idx2word[len(idx2word)] = word\n",
    "            word2idx[word] = len(idx2word) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pre-computed cosine similarity matrix from cos_sim_counter_fitting.npy\n"
     ]
    }
   ],
   "source": [
    "print('Load pre-computed cosine similarity matrix from {}'.format('cos_sim_counter_fitting.npy'))\n",
    "cos_sim = np.load('cos_sim_counter_fitting.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_most_similar_words_batch(src_words, sim_mat, idx2word, ret_count=10, threshold=0.):\n",
    "    \"\"\"\n",
    "    Given a list of source words (their indices), a similarity matrix, and an index-to-word mapping,\n",
    "    this function returns the top `ret_count` similar words for each source word, filtered by a given threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    - src_words: List of source word indices.\n",
    "    - sim_mat: Similarity matrix of shape (vocab_size, vocab_size).\n",
    "    - idx2word: A mapping from word index to actual word.\n",
    "    - ret_count: Number of top similar words to return for each source word.\n",
    "    - threshold: A similarity threshold to filter out words.\n",
    "    \n",
    "    Returns:\n",
    "    - sim_words: A list of lists containing similar words for each source word.\n",
    "    - sim_values: A list of lists containing similarity values for each word in sim_words.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 对于每个src_word，找到其与其他所有单词的相似度排名（从高到低）\n",
    "    sim_order = np.argsort(-sim_mat[src_words, :])[:, 1:1 + ret_count]\n",
    "    \n",
    "    sim_words, sim_values = [], []  # 初始化列表以保存结果\n",
    "\n",
    "    # 遍历src_words的每个词\n",
    "    for idx, src_word in enumerate(src_words):\n",
    "        # 获取对应src_word的相似度值\n",
    "        sim_value = sim_mat[src_word][sim_order[idx]]\n",
    "        \n",
    "        # 根据阈值筛选出大于等于threshold的相似度值\n",
    "        mask = sim_value >= threshold\n",
    "        \n",
    "        # 使用mask获取单词和其相似度值\n",
    "        sim_word, sim_value = sim_order[idx][mask], sim_value[mask]\n",
    "        \n",
    "        # 将单词索引转换为实际的单词\n",
    "        sim_word = [idx2word[id] for id in sim_word]\n",
    "        \n",
    "        # 保存结果\n",
    "        sim_words.append(sim_word)\n",
    "        sim_values.append(sim_value)\n",
    "\n",
    "    return sim_words, sim_values  # 返回相似单词及其相似度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ls = \"Whoever wrote the screenplay for this movie obviously never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\".split()\n",
    "perturb_idxes = [7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_perturb = [(idx, text_ls[idx]) for idx in perturb_idxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 'obviously')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_perturb_idx = [word2idx[word] for idx, word in words_perturb if word in word2idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_words, _ = pick_most_similar_words_batch(words_perturb_idx, cos_sim, idx2word, 10, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['evidently',\n",
       "  'clearly',\n",
       "  'manifestly',\n",
       "  'naturally',\n",
       "  'patently',\n",
       "  'apparently',\n",
       "  'plainly',\n",
       "  'definitely',\n",
       "  'surely',\n",
       "  'undoubtedly']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_all = []\n",
    "for idx, word in words_perturb:\n",
    "    if word in word2idx:\n",
    "        synonyms = synonym_words.pop(0)\n",
    "        if synonyms:\n",
    "            synonyms_all.append((idx, synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7,\n",
       "  ['evidently',\n",
       "   'clearly',\n",
       "   'manifestly',\n",
       "   'naturally',\n",
       "   'patently',\n",
       "   'apparently',\n",
       "   'plainly',\n",
       "   'definitely',\n",
       "   'surely',\n",
       "   'undoubtedly'])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prime = text_ls\n",
    "len_text = len(text_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Whoever', 'wrote', 'the', 'screenplay', 'for', 'this', 'movie', 'evidently', 'never', 'consulted', 'any', 'books', 'about', 'Lucille', 'Ball,', 'especially', 'her', 'autobiography.', \"I've\", 'never', 'seen', 'so', 'many', 'mistakes', 'in', 'a', 'biopic,', 'ranging', 'from', 'her', 'early', 'years', 'in', 'Celoron', 'and', 'Jamestown', 'to', 'her', 'later', 'years', 'with', 'Desi.', 'I', 'could', 'write', 'a', 'whole', 'list', 'of', 'factual', 'errors,', 'but', 'it', 'would', 'go', 'on', 'for', 'pages.', 'In', 'all,', 'I', 'believe', 'that', 'Lucille', 'Ball', 'is', 'one', 'of', 'those', 'inimitable', 'people', 'who', 'simply', 'cannot', 'be', 'portrayed', 'by', 'anyone', 'other', 'than', 'themselves.', 'If', 'I', 'were', 'Lucie', 'Arnaz', 'and', 'Desi,', 'Jr.,', 'I', 'would', 'be', 'irate', 'at', 'how', 'many', 'mistakes', 'were', 'made', 'in', 'this', 'film.', 'The', 'filmmakers', 'tried', 'hard,', 'but', 'the', 'movie', 'seems', 'awfully', 'sloppy', 'to', 'me.'], ['Whoever', 'wrote', 'the', 'screenplay', 'for', 'this', 'movie', 'clearly', 'never', 'consulted', 'any', 'books', 'about', 'Lucille', 'Ball,', 'especially', 'her', 'autobiography.', \"I've\", 'never', 'seen', 'so', 'many', 'mistakes', 'in', 'a', 'biopic,', 'ranging', 'from', 'her', 'early', 'years', 'in', 'Celoron', 'and', 'Jamestown', 'to', 'her', 'later', 'years', 'with', 'Desi.', 'I', 'could', 'write', 'a', 'whole', 'list', 'of', 'factual', 'errors,', 'but', 'it', 'would', 'go', 'on', 'for', 'pages.', 'In', 'all,', 'I', 'believe', 'that', 'Lucille', 'Ball', 'is', 'one', 'of', 'those', 'inimitable', 'people', 'who', 'simply', 'cannot', 'be', 'portrayed', 'by', 'anyone', 'other', 'than', 'themselves.', 'If', 'I', 'were', 'Lucie', 'Arnaz', 'and', 'Desi,', 'Jr.,', 'I', 'would', 'be', 'irate', 'at', 'how', 'many', 'mistakes', 'were', 'made', 'in', 'this', 'film.', 'The', 'filmmakers', 'tried', 'hard,', 'but', 'the', 'movie', 'seems', 'awfully', 'sloppy', 'to', 'me.'], ['Whoever', 'wrote', 'the', 'screenplay', 'for', 'this', 'movie', 'manifestly', 'never', 'consulted', 'any', 'books', 'about', 'Lucille', 'Ball,', 'especially', 'her', 'autobiography.', \"I've\", 'never', 'seen', 'so', 'many', 'mistakes', 'in', 'a', 'biopic,', 'ranging', 'from', 'her', 'early', 'years', 'in', 'Celoron', 'and', 'Jamestown', 'to', 'her', 'later', 'years', 'with', 'Desi.', 'I', 'could', 'write', 'a', 'whole', 'list', 'of', 'factual', 'errors,', 'but', 'it', 'would', 'go', 'on', 'for', 'pages.', 'In', 'all,', 'I', 'believe', 'that', 'Lucille', 'Ball', 'is', 'one', 'of', 'those', 'inimitable', 'people', 'who', 'simply', 'cannot', 'be', 'portrayed', 'by', 'anyone', 'other', 'than', 'themselves.', 'If', 'I', 'were', 'Lucie', 'Arnaz', 'and', 'Desi,', 'Jr.,', 'I', 'would', 'be', 'irate', 'at', 'how', 'many', 'mistakes', 'were', 'made', 'in', 'this', 'film.', 'The', 'filmmakers', 'tried', 'hard,', 'but', 'the', 'movie', 'seems', 'awfully', 'sloppy', 'to', 'me.'], ['Whoever', 'wrote', 'the', 'screenplay', 'for', 'this', 'movie', 'naturally', 'never', 'consulted', 'any', 'books', 'about', 'Lucille', 'Ball,', 'especially', 'her', 'autobiography.', \"I've\", 'never', 'seen', 'so', 'many', 'mistakes', 'in', 'a', 'biopic,', 'ranging', 'from', 'her', 'early', 'years', 'in', 'Celoron', 'and', 'Jamestown', 'to', 'her', 'later', 'years', 'with', 'Desi.', 'I', 'could', 'write', 'a', 'whole', 'list', 'of', 'factual', 'errors,', 'but', 'it', 'would', 'go', 'on', 'for', 'pages.', 'In', 'all,', 'I', 'believe', 'that', 'Lucille', 'Ball', 'is', 'one', 'of', 'those', 'inimitable', 'people', 'who', 'simply', 'cannot', 'be', 'portrayed', 'by', 'anyone', 'other', 'than', 'themselves.', 'If', 'I', 'were', 'Lucie', 'Arnaz', 'and', 'Desi,', 'Jr.,', 'I', 'would', 'be', 'irate', 'at', 'how', 'many', 'mistakes', 'were', 'made', 'in', 'this', 'film.', 'The', 'filmmakers', 'tried', 'hard,', 'but', 'the', 'movie', 'seems', 'awfully', 'sloppy', 'to', 'me.'], ['Whoever', 'wrote', 'the', 'screenplay', 'for', 'this', 'movie', 'patently', 'never', 'consulted', 'any', 'books', 'about', 'Lucille', 'Ball,', 'especially', 'her', 'autobiography.', \"I've\", 'never', 'seen', 'so', 'many', 'mistakes', 'in', 'a', 'biopic,', 'ranging', 'from', 'her', 'early', 'years', 'in', 'Celoron', 'and', 'Jamestown', 'to', 'her', 'later', 'years', 'with', 'Desi.', 'I', 'could', 'write', 'a', 'whole', 'list', 'of', 'factual', 'errors,', 'but', 'it', 'would', 'go', 'on', 'for', 'pages.', 'In', 'all,', 'I', 'believe', 'that', 'Lucille', 'Ball', 'is', 'one', 'of', 'those', 'inimitable', 'people', 'who', 'simply', 'cannot', 'be', 'portrayed', 'by', 'anyone', 'other', 'than', 'themselves.', 'If', 'I', 'were', 'Lucie', 'Arnaz', 'and', 'Desi,', 'Jr.,', 'I', 'would', 'be', 'irate', 'at', 'how', 'many', 'mistakes', 'were', 'made', 'in', 'this', 'film.', 'The', 'filmmakers', 'tried', 'hard,', 'but', 'the', 'movie', 'seems', 'awfully', 'sloppy', 'to', 'me.'], ['Whoever', 'wrote', 'the', 'screenplay', 'for', 'this', 'movie', 'apparently', 'never', 'consulted', 'any', 'books', 'about', 'Lucille', 'Ball,', 'especially', 'her', 'autobiography.', \"I've\", 'never', 'seen', 'so', 'many', 'mistakes', 'in', 'a', 'biopic,', 'ranging', 'from', 'her', 'early', 'years', 'in', 'Celoron', 'and', 'Jamestown', 'to', 'her', 'later', 'years', 'with', 'Desi.', 'I', 'could', 'write', 'a', 'whole', 'list', 'of', 'factual', 'errors,', 'but', 'it', 'would', 'go', 'on', 'for', 'pages.', 'In', 'all,', 'I', 'believe', 'that', 'Lucille', 'Ball', 'is', 'one', 'of', 'those', 'inimitable', 'people', 'who', 'simply', 'cannot', 'be', 'portrayed', 'by', 'anyone', 'other', 'than', 'themselves.', 'If', 'I', 'were', 'Lucie', 'Arnaz', 'and', 'Desi,', 'Jr.,', 'I', 'would', 'be', 'irate', 'at', 'how', 'many', 'mistakes', 'were', 'made', 'in', 'this', 'film.', 'The', 'filmmakers', 'tried', 'hard,', 'but', 'the', 'movie', 'seems', 'awfully', 'sloppy', 'to', 'me.'], ['Whoever', 'wrote', 'the', 'screenplay', 'for', 'this', 'movie', 'plainly', 'never', 'consulted', 'any', 'books', 'about', 'Lucille', 'Ball,', 'especially', 'her', 'autobiography.', \"I've\", 'never', 'seen', 'so', 'many', 'mistakes', 'in', 'a', 'biopic,', 'ranging', 'from', 'her', 'early', 'years', 'in', 'Celoron', 'and', 'Jamestown', 'to', 'her', 'later', 'years', 'with', 'Desi.', 'I', 'could', 'write', 'a', 'whole', 'list', 'of', 'factual', 'errors,', 'but', 'it', 'would', 'go', 'on', 'for', 'pages.', 'In', 'all,', 'I', 'believe', 'that', 'Lucille', 'Ball', 'is', 'one', 'of', 'those', 'inimitable', 'people', 'who', 'simply', 'cannot', 'be', 'portrayed', 'by', 'anyone', 'other', 'than', 'themselves.', 'If', 'I', 'were', 'Lucie', 'Arnaz', 'and', 'Desi,', 'Jr.,', 'I', 'would', 'be', 'irate', 'at', 'how', 'many', 'mistakes', 'were', 'made', 'in', 'this', 'film.', 'The', 'filmmakers', 'tried', 'hard,', 'but', 'the', 'movie', 'seems', 'awfully', 'sloppy', 'to', 'me.'], ['Whoever', 'wrote', 'the', 'screenplay', 'for', 'this', 'movie', 'definitely', 'never', 'consulted', 'any', 'books', 'about', 'Lucille', 'Ball,', 'especially', 'her', 'autobiography.', \"I've\", 'never', 'seen', 'so', 'many', 'mistakes', 'in', 'a', 'biopic,', 'ranging', 'from', 'her', 'early', 'years', 'in', 'Celoron', 'and', 'Jamestown', 'to', 'her', 'later', 'years', 'with', 'Desi.', 'I', 'could', 'write', 'a', 'whole', 'list', 'of', 'factual', 'errors,', 'but', 'it', 'would', 'go', 'on', 'for', 'pages.', 'In', 'all,', 'I', 'believe', 'that', 'Lucille', 'Ball', 'is', 'one', 'of', 'those', 'inimitable', 'people', 'who', 'simply', 'cannot', 'be', 'portrayed', 'by', 'anyone', 'other', 'than', 'themselves.', 'If', 'I', 'were', 'Lucie', 'Arnaz', 'and', 'Desi,', 'Jr.,', 'I', 'would', 'be', 'irate', 'at', 'how', 'many', 'mistakes', 'were', 'made', 'in', 'this', 'film.', 'The', 'filmmakers', 'tried', 'hard,', 'but', 'the', 'movie', 'seems', 'awfully', 'sloppy', 'to', 'me.'], ['Whoever', 'wrote', 'the', 'screenplay', 'for', 'this', 'movie', 'surely', 'never', 'consulted', 'any', 'books', 'about', 'Lucille', 'Ball,', 'especially', 'her', 'autobiography.', \"I've\", 'never', 'seen', 'so', 'many', 'mistakes', 'in', 'a', 'biopic,', 'ranging', 'from', 'her', 'early', 'years', 'in', 'Celoron', 'and', 'Jamestown', 'to', 'her', 'later', 'years', 'with', 'Desi.', 'I', 'could', 'write', 'a', 'whole', 'list', 'of', 'factual', 'errors,', 'but', 'it', 'would', 'go', 'on', 'for', 'pages.', 'In', 'all,', 'I', 'believe', 'that', 'Lucille', 'Ball', 'is', 'one', 'of', 'those', 'inimitable', 'people', 'who', 'simply', 'cannot', 'be', 'portrayed', 'by', 'anyone', 'other', 'than', 'themselves.', 'If', 'I', 'were', 'Lucie', 'Arnaz', 'and', 'Desi,', 'Jr.,', 'I', 'would', 'be', 'irate', 'at', 'how', 'many', 'mistakes', 'were', 'made', 'in', 'this', 'film.', 'The', 'filmmakers', 'tried', 'hard,', 'but', 'the', 'movie', 'seems', 'awfully', 'sloppy', 'to', 'me.'], ['Whoever', 'wrote', 'the', 'screenplay', 'for', 'this', 'movie', 'undoubtedly', 'never', 'consulted', 'any', 'books', 'about', 'Lucille', 'Ball,', 'especially', 'her', 'autobiography.', \"I've\", 'never', 'seen', 'so', 'many', 'mistakes', 'in', 'a', 'biopic,', 'ranging', 'from', 'her', 'early', 'years', 'in', 'Celoron', 'and', 'Jamestown', 'to', 'her', 'later', 'years', 'with', 'Desi.', 'I', 'could', 'write', 'a', 'whole', 'list', 'of', 'factual', 'errors,', 'but', 'it', 'would', 'go', 'on', 'for', 'pages.', 'In', 'all,', 'I', 'believe', 'that', 'Lucille', 'Ball', 'is', 'one', 'of', 'those', 'inimitable', 'people', 'who', 'simply', 'cannot', 'be', 'portrayed', 'by', 'anyone', 'other', 'than', 'themselves.', 'If', 'I', 'were', 'Lucie', 'Arnaz', 'and', 'Desi,', 'Jr.,', 'I', 'would', 'be', 'irate', 'at', 'how', 'many', 'mistakes', 'were', 'made', 'in', 'this', 'film.', 'The', 'filmmakers', 'tried', 'hard,', 'but', 'the', 'movie', 'seems', 'awfully', 'sloppy', 'to', 'me.']]\n"
     ]
    }
   ],
   "source": [
    "for idx, synonyms in synonyms_all:\n",
    "    new_texts = [text_prime[:idx] + [synonym] + text_prime[min(idx + 1, len_text):] for synonym in synonyms]\n",
    "    print(new_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 加载预训练的BERT模型和tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_sentence(sentence, mask_idx, tokenizer):\n",
    "    \"\"\"Inserts a [MASK] token at the specified index of the sentence.\"\"\"\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens[mask_idx] = '[MASK]'\n",
    "    return tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "# 2. 标记化句子并获得[MASK]的位置\n",
    "sentence = \"Whoever wrote the screenplay for this movie obviously never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\"\n",
    "\n",
    "new_sentence = mask_sentence(sentence, mask_idx=7, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = tokenizer(new_sentence, return_tensors=\"pt\")\n",
    "mask_idx = torch.where(inputs[\"input_ids\"][0] == tokenizer.mask_token_id)[0].item()\n",
    "\n",
    "# 3. 使用BERT预测[MASK]位置的概率分布\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = outputs.logits[0, mask_idx].softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 'evidently' being the masked token: 0.0001021026\n",
      "Probability of 'clearly' being the masked token: 0.0001295633\n",
      "Probability of 'manifestly' being the masked token: 0.0000004529\n",
      "Probability of 'naturally' being the masked token: 0.0000256165\n",
      "Probability of 'patently' being the masked token: 0.0000004529\n",
      "Probability of 'apparently' being the masked token: 0.0006515997\n",
      "Probability of 'plainly' being the masked token: 0.0000012570\n",
      "Probability of 'definitely' being the masked token: 0.0002907286\n",
      "Probability of 'surely' being the masked token: 0.0001693258\n",
      "Probability of 'undoubtedly' being the masked token: 0.0000840849\n"
     ]
    }
   ],
   "source": [
    "word_mask_probability = {}\n",
    "\n",
    "for idx, word_list in synonyms_all:\n",
    "    for word in word_list:\n",
    "        word_id = tokenizer.convert_tokens_to_ids(word)\n",
    "        word_probability = predictions[word_id].item()\n",
    "        word_mask_probability[word] = word_probability\n",
    "        print(f\"Probability of '{word}' being the masked token: {word_probability:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evidently': 0.0701647586899207,\n",
       " 'clearly': 0.08903565516000746,\n",
       " 'manifestly': 0.0003112191691114321,\n",
       " 'naturally': 0.017603581241432697,\n",
       " 'patently': 0.0003112191691114321,\n",
       " 'apparently': 0.4477781690909761,\n",
       " 'plainly': 0.0008638359003986049,\n",
       " 'definitely': 0.19978818133903484,\n",
       " 'surely': 0.11636039484706538,\n",
       " 'undoubtedly': 0.05778298539294134}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 计算字典中所有值的和\n",
    "\n",
    "total_value = sum(word_mask_probability.values())\n",
    "# 2. 使用这个和来对每个值进行归一化\n",
    "normalized_data = {k: v / total_value for k, v in word_mask_probability.items()}\n",
    "\n",
    "normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = dict(sorted(normalized_data.items(), key=lambda item: item[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apparently': 0.4477781690909761,\n",
       " 'definitely': 0.19978818133903484,\n",
       " 'surely': 0.11636039484706538,\n",
       " 'clearly': 0.08903565516000746,\n",
       " 'evidently': 0.0701647586899207,\n",
       " 'undoubtedly': 0.05778298539294134,\n",
       " 'naturally': 0.017603581241432697,\n",
       " 'plainly': 0.0008638359003986049,\n",
       " 'manifestly': 0.0003112191691114321,\n",
       " 'patently': 0.0003112191691114321}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Whoever wrote the screenplay for this movie apparently never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\",\n",
       " \"Whoever wrote the screenplay for this movie definitely never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\",\n",
       " \"Whoever wrote the screenplay for this movie surely never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\",\n",
       " \"Whoever wrote the screenplay for this movie clearly never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\",\n",
       " \"Whoever wrote the screenplay for this movie evidently never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\",\n",
       " \"Whoever wrote the screenplay for this movie undoubtedly never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\",\n",
       " \"Whoever wrote the screenplay for this movie naturally never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\",\n",
       " \"Whoever wrote the screenplay for this movie plainly never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\",\n",
       " \"Whoever wrote the screenplay for this movie manifestly never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\",\n",
       " \"Whoever wrote the screenplay for this movie patently never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bert_score\n",
    "def replace_word_at_index(sentence, index, replacement_dict):\n",
    "    \"\"\"Replace the word at the specified index if it exists in the replacement_dict.\"\"\"\n",
    "    tokens = sentence.split()  # Split sentence into tokens\n",
    "    # if tokens[index] in replacement_dict:\n",
    "    #     tokens[index] = replacement_dict[tokens[index]]  # Replace the word at the specified index\n",
    "    word_replace = []\n",
    "\n",
    "    for word in replacement_dict.keys():\n",
    "        tokens[index] = word\n",
    "        word_replace.append(\" \".join(tokens))\n",
    "        \n",
    "    return word_replace# Join tokens back into a sentence\n",
    "\n",
    "def compute_bertscore(candidates, references, lang=\"en\", verbose=True, model_type=\"bert-base-uncased\"):\n",
    "\n",
    "    candidates = [candidates]\n",
    "    P, R, F1 = bert_score.score(candidates, references, lang=lang, verbose=verbose, model_type=model_type)\n",
    "    \n",
    "    return P, R, F1\n",
    "# Example usage:\n",
    "candidates = \"Whoever wrote the screenplay for this movie obviously never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\"\n",
    "references = replace_word_at_index(sentence=candidates, index=7, replacement_dict=sorted_data)\n",
    "references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d334e116534d21b13eda14777c9c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86dd75fec1af4a599dcb7ab9d9ae8df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.84 seconds, 1.19 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d841459b9ca4d2e97374e619d2dd429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adc229be1a8460c864d4b4eb2c0bb23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 64.60 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5047e63b88f244d1a918a84f9ae336dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2618bf374eb745b181e557d40c41cae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 64.42 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796ffa05b8e640548f29a269a3092383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33b8ba8b42545ea9a6b40e700a5bd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 55.51 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934f891135db4a1d8614914d6c75a38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7d25978cf34c05a6e5ee7dca0dc0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 65.38 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1440c9bcaafa46fb99b1cd805219f4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c490a879f74d5e849827097fef16eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 58.52 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f7f4e7afaa46d3ac65320c0344bd75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2c93fbb0cc4d7998524c22aa3b6b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 65.48 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1416a21ecbf0410da726eb1973adcbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c046f039a2994f2db75c713aaf17b140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 64.01 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3a8c6f593941119a557c02fab04a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db62e85970704f65832b791e88a9566f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 63.39 sentences/sec\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9923e5f1c38d490bb6823d4bd4243e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49b69525e284fcea53bd0befe709393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 62.89 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "bertscore = {}\n",
    "for idx, reference in enumerate(references):\n",
    "    reference = [reference]\n",
    "\n",
    "    P, R, F1 = compute_bertscore(candidates, reference)\n",
    "    bertscore[list(sorted_data.keys())[idx]] = F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 计算字典中所有值的和\n",
    "\n",
    "total_value = sum(bertscore.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apparently': tensor([0.1001]),\n",
       " 'definitely': tensor([0.1001]),\n",
       " 'surely': tensor([0.1001]),\n",
       " 'clearly': tensor([0.1002]),\n",
       " 'evidently': tensor([0.1001]),\n",
       " 'undoubtedly': tensor([0.1001]),\n",
       " 'naturally': tensor([0.1000]),\n",
       " 'plainly': tensor([0.1000]),\n",
       " 'manifestly': tensor([0.0997]),\n",
       " 'patently': tensor([0.0997])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 使用这个和来对每个值进行归一化\n",
    "normalized_data = {k: v / total_value for k, v in bertscore.items()}\n",
    "\n",
    "normalized_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adver_attack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
